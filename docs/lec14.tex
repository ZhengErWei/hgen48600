% -----------------------------*- LaTeX -*------------------------------
\documentclass[12pt]{report}
\usepackage{scribe_hgen486}
\usepackage{textcomp}
\usepackage{flexisym}
\usepackage{amsmath}
\usepackage{stackengine}
\usepackage{tikz}
\usetikzlibrary{shapes.geometric}
\usepackage{tikz}
\usetikzlibrary{graphs, graphdrawing}
\usegdlibrary{layered}

\begin{document}

\scribe{Lauren E. Blake}		% required
\lecturenumber{14}			% required, must be a number
\lecturedate{February 18}		% required, omit year
\lecturer{Matthew Stephens} 

\maketitle

% please leave this comment 
\framebox[.95\textwidth]{\parbox{.93\textwidth}{ {{\bf Note:}} These
lecture notes are still rough, and have only have been mildly
proofread.  }}
\vspace*{.1in}


% feel free to delete content below this line 
% ----------------------------------------------------------------------

\section{Class notes}

\begin{enumerate}

\item There are two vignettes on the multivariate normal on Matthew's website: 

https://github.com/stephens999/fiveMinuteStats/tree/gh-pages/analysis
\item The reading for this lecture can be found in Chapter 10 of the Ross textbook. 

\end{enumerate}

\section{Background}

If $X_{1}$ and $X_{2}$ are independent normals ({\raise.17ex\hbox{$\scriptstyle\sim$}} N(0, 1) ), then a$X_{1}$ + b$X_{2}$ {\raise.17ex\hbox{$\scriptstyle\sim$}} N(0, $a^{2}$ + $b^{2}$)

\section{Introduction to the multivariate normal}

\begin{itemize}

\item There are many ways to definite the multivariate normal, but the definition from Wikipedia states that Vector X = ($X_{1}$, \ldots $X_{r}$) is r-variate normal or ``r-dimensional multivariate normal'' if every alternate linear combination of X, $\lambda_{1}$$X_{1}$ + \ldots + $\lambda_{r}$$X_{r}$, is univariate normal (as long as $\lambda$ $\neq$ 0).

\item Suppose Z = ($Z_{1}$, \ldots $Z_{n}$) are iid {\raise.17ex\hbox{$\scriptstyle\sim$}} N(0,1). Let A by an r x n matrix and $\mu$ be an r-vector. Then X = $\mu$ + AZ is a multivariate normal with mean E($X_{j}$) = $\mu_{j}$ and covariance matrix Cov($X_{i}$, $X_{j}$) = (A A$\textprime$)$ _{ij}$.

\item For example, Cov($X_{1}$, $X_{1}$) = Var($X_{1}$) so the diagonal of the covariance matrix is one. 

\end{itemize}

\section{An example}

There is an example in the vignette, $Z_{1}$, $Z_{2}$, $Z_{3}$ where 
 \[A=
  \begin{bmatrix}
    1 & 1 & 0 \\
    1 & 0 & 1
  \end{bmatrix}
\]

and where $X_{1}$ = $Z_{1}$ and $Z_{2}$ and $X_{2}$ = $Z_{1}$ + $Z_{3}$.

\bigskip
From this, we get X = AZ {\raise.17ex\hbox{$\scriptstyle\sim$}} N(0, $\Sigma$ = A A$\textprime$) 

\begin{itemize}

\bigskip \item Note that  $\Sigma$ = A A$\textprime$ = \[  \begin{bmatrix}
    2 & 1  \\
    1 & 2
  \end{bmatrix}
\] 

\item The covariance comes from the fact that they share 2 elements ($Z_{1}$) and don't share two elements ($Z_{2}$ and $Z_{3}$).

\item Corr = \[\frac{Cov}{\sqrt(Var)\sqrt(Var)} \] = \[\frac{1}{2} \] =  \[\frac{Cov}{\sqrt(2)\sqrt(2)} \]. 

\bigskip
This gives us a 3D ellipse that is tilted towards the right:

\begin{tikzpicture}

\draw [help lines] (-2, -2) grid (2, 2);
\draw[rotate=-45] (0, 0) ellipse (0.5cm and 1.5cm);
\draw[rotate=-45] (0, 0) ellipse (0.3cm and 0.7cm);

\draw[<->] (-2.0, 0) -- (2.0, 0) node[right]{\footnotesize x};
\draw[<->] (0, -1.5) -- (0, 1.5) node[above]{\footnotesize y};

\end{tikzpicture}

\item If we were to change the covariance matrix so A A$\textprime$ =  \[  \begin{bmatrix}
    2 & -1  \\
    -1 & 2
  \end{bmatrix}
\] then we would have an 3D ellipse with the same shape but tilted to the left:

\begin{tikzpicture}

\draw [help lines] (-2, -2) grid (2, 2);
\draw[rotate=45] (0, 0) ellipse (0.5cm and 1.5cm);
\draw[rotate=45] (0, 0) ellipse (0.3cm and 0.7cm);

\draw[<->] (-2.0, 0) -- (2.0, 0) node[right]{\footnotesize x};
\draw[<->] (0, -1.5) -- (0, 1.5) node[above]{\footnotesize y};

\end{tikzpicture}

\end{itemize}
 
 \section{Simulating from the multivariate}
 
 \begin{itemize}
 
 \item We can use the fact that a multivariate normal are linear combinations of univariate normals
 
 \item For $N_{r}$($\mu$, $\Sigma_{r}$), you can simulate from this provided that you can find A such that A A$\textprime$ = $\Sigma$.
 
 \item Note: $\Sigma$ must be symmetric to hold. This means that $\Sigma$ must be positive and semi-definite (so it has all non-negative eigenvectors).
 
 \item Inverting matrices can be unstable, so we need at least one other way to find the inverse of a matrix. In the Cholesky decomposition of $\Sigma$ in the lower triangular matrix L has a value such that L L$\textprime$ = $\Sigma$. This allows use to find L and the from that find Z and then find X. We can then add $\mu$ if we are so inclined. 
 
 \item To find the inverse of a matrix in R, use the command \textit{chol2 inv (chol($\Sigma$))}.
 
 \item It is important to note that we use this because inverting matrices can be unstable.
 
 \item We don't always have a density but if it is invertable (has all positive eigenvalues), then the density of the multivariate normal is the following:
 
 \begin{eqnarray}
p(x) = (\frac{1}{\sqrt{(2\pi)|\Sigma|}})^{\frac{r}{2}}  exp(-\frac{1}{2}(x-\mu)'\Sigma^{-1}(x-\mu))
\end{eqnarray}
\\
 
 \end{itemize}

\subsection{Maximizing p(x) for the multivariate normal}

\begin{itemize}

\item It is not straightforward to show how to maximize p(x).

\item If $X_{1}$ \ldots $X_{n}$ (that are iid) {\raise.17ex\hbox{$\scriptstyle\sim$}} $N_{r}$($\mu$, $\Sigma$), then the MLE for $\mu$ and $\Sigma$ is $\hat{\mu}_{ij}$ = $\frac{1}{n}$ $\sum\limits_{i}$ $x_{ij}$ . This is the average of vectors. 

\item Here, we are going to average j matrices: $\hat{\Sigma}$ = $\frac{1}{n}$ $\sum\limits_{i}$ (\underline{$x_{i}$} - \underline{$\hat{\mu}$})(\underline{$x_{i}$}  - \underline{$\hat{\mu}$})$\textprime$

\item Therefore, $\hat{\Sigma}$ = $\frac{1}{n}$ $\sum\limits_{i}$ ($x_{ij}$ - $\hat{\mu}_{j}$)(\underline{$x_{ik}$}  - $\hat{\mu}_{k}$)

\item The likelihood:  \begin{eqnarray}
L(\mu, \Sigma) = \prod_{i} (\frac{1}{\sqrt{(2\pi)|\Sigma|}})^{\frac{r}{2}} exp(-\frac{1}{2}(x_{i}-\mu)'\Sigma^{-1}(x_{i}-\mu))
\end{eqnarray}
\\

\end{itemize}

\section{Introduction to Gaussian Processes and Brownian Models}

\begin{itemize}

\item If $X_{1}$, $X_{2}$ \ldots is a Markov chain, then $X_{t+1}$ $\mid$ $X_{t}$ {\raise.17ex\hbox{$\scriptstyle\sim$}} N($X_{t}$, 1). It is a normal Markov chain where be begin at the origin (0,0) and $X_{1}$ {\raise.17ex\hbox{$\scriptstyle\sim$}} N(0,1). As a result, we might call the a random walk, where each time we go an amount from a normal distribution. 

\item If we were to stop at X = 1000, then it would be a 1000-D multivariate normal where 

\bigskip
$X_{1}$  = $Z_{1}$ 

$X_{2}$  = $X_{2}$  + $Z_{1}$ = $Z_{1}$  + $Z_{2}$

 $X_{3}$  = $X_{2}$  + $Z_{3}$ = $Z_{1}$  + $Z_{2}$ + $Z_{3}$
 
 \bigskip
 so \underline{X} = AZ.

\item Note: With a multivariate normal, they are uncorrelated if and only if they are independent.

\item In the example \underline{X} = AZ, the points closer together are going to appear more correlated than points that are farther away. 

\item $\Sigma$ = A A$\textprime$ =  \[  \begin{bmatrix}
    1 & 1 & 1 & 1 & 1  \\
    1 & 2 & 2 & 2 & 2 \\
    1 & 2 & 3 & 3 & 3 \\
    1 & 2 & 3 & 4 & 4 \\
    1 & 2 & 3 & 4 & 5
  \end{bmatrix}
	\] 

\item But this is really interesting what you take the inverse $\Omega$ = $\Sigma^{-1}$ =  \[  \begin{bmatrix}
    2 & -1 & 0 & 0 & 0  \\
    -1 & 2 & -1 & 0 & 0 \\
    0 & -1 & 2 & -1 & 0 \\
    0 & 0 & -1 & 2 & -1 \\
    0 & 0 & 0 & -1  & 2
  \end{bmatrix}
	\] 

where $\Omega$ is the precision matrix.

\item The structure of the inverse of the covariance matrix can indicate that this is a Markov chain because they have the following special property:

\bigskip
\begin{enumerate}

\item If X is multivariate normal, \underline{X} {\raise.17ex\hbox{$\scriptstyle\sim$}} $N_{r}$(0, $\Omega^{-1}$)

\item $\Omega^{ij}$ = 0 if and only if $X_{i}$ and $X_{j}$ are conditionally independent given all other Xs

\item $\Sigma_{ij}$ = 0 if and only if $X_{i}$ and $X_{j}$ are independent

\end{enumerate}

\end{itemize}

\section{(Undirected) Gaussian graphical models}

\subsection{Introduction}

\begin{itemize}

\item For any precision matrix, you want to make a connection if there is a non-zero covariance. This has many applications, including a gene network. For any precision matrix, you would make a connection between genes that have a non-zero covariance. 

\item The advantage of Gaussian graphical models is that you decrease the number of parameters that you are trying to estimate

\item In the gene network example, you may have hub genes or master regulators that control a bunch of other genes. The signal from the controlled genes are observed, but the signal from the hub genes are not observed. Given that these are unobserved, there are no conditional independents. For example, the precision matrix for $X_{1}$, $X_{2}$, \ldots (observed signals) is dense but the precision matrix with $F_{1}$ \ldots (from the unobserved signals) and $X_{1}$ \ldots (from the observed signals) is sparse. Removing an unobserved value won't drastically change a covariance matrix (it will change at only 1 row), but it will change the precision matrix everywhere.

\tikz [gr/.style={gray!50}, font=\bfseries]
\graph [layered layout] {
    % A and F are horizontally aligned if you also set weight=0.5 for A -- C.
    F1 -> X1,
    F1 -> X3,
    F1 -> X5,
    F2 -> X1,
    F2 -> X2,
    F2 -> X4,
    F2 -> X7,
    F3 -> X4,
    F3 -> X6,
    F3 -> X8,
    
    
}; 

\item It is appealing for genetics and other applications, however, we are plagued by noisy data (e.g. expression data from RNA-seq). Since the true value of the measurement is unknown, we have to treat everything as unobserved, which is problematic. 

\end{itemize}

\subsection{Utilization}

\begin{itemize}

\item Assume the observations have come to a relatively small number of factors so that the relationships between the observations are relatively simple.

\begin{equation}
\mathbf{X} = LF + E
\end{equation}
\\

where L represents the loadings, F represents the factors, and E represents the error terms. \textbf{X} has dimensions p x n. L has dimensions p x k and F has dimensions k x n where k is a small number of factors. E has dimensions p x n. 

\item In a genetics application, F is individual transcription factors, where each TF has different effects across the observations (e.g. TF binding sites near genes). 

\item If we assume that k factors F are independent, $N_{n}$(0, I) and rows of E are independent  $N_{n}$(0, $\psi$), then $X_{i}$ {\raise.17ex\hbox{$\scriptstyle\sim$}} N(0, L L$\textprime$ + $\psi$). In words, $X_{i}$ is represented by a multivariate normal with rank k matrix and diagonal, $\psi$.

\item We are making 3 assumptions in order to decrease the number of parameters to be estimated:

\begin{enumerate}

\item The covariance matrix is sparse.
\item The inverse covariance matrix is sparse. 
\item The covariance matrix is rank k normal with a diagonal. This is also known as factor models

\end{enumerate}

\end{itemize}





\end{document}

%%% Local Variables
%%% TeX-engine: luatex
%%% End